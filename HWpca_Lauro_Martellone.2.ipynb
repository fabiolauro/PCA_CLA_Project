{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6347f1d2-5cb0-4032-b637-4730e9b9383a",
   "metadata": {},
   "source": [
    "# Computational Linear Algebra: PCA Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24f22b-a92f-4898-b26d-e4ab6d49c3d9",
   "metadata": {},
   "source": [
    "## Starting Code-Cell \n",
    "### Attention: DO NOT CHANGE THE CODE INSIDE THE FOLLOWING CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905c56f-029b-4d04-9e68-739362350bd8",
   "metadata": {},
   "source": [
    "## Initialization:\n",
    "Fill the missing values in this text box and in the following code-cell.\n",
    "\n",
    "**Academic Year:** 2024/2025\n",
    "\n",
    "### Team Members (Alphabetical Order):\n",
    "1. Lauro, Fabio (318121);\n",
    "2. Martellone, Lorenzo (317331)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c068e99-0c8c-433f-be0b-f41534dc8a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:13:51.176069Z",
     "start_time": "2024-12-21T14:13:51.156004Z"
    }
   },
   "outputs": [],
   "source": [
    "StudentID1 = 318121\n",
    "StudentID2 = 317331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84734de0-62ad-4c39-b4b4-886696d3a3f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:13:53.875816Z",
     "start_time": "2024-12-21T14:13:53.770922Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############## DO NOT CHANGE THE CODE IN THIS CELL #################\n",
    "####################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "var_entertainment_feat_types = ['Interests', 'Movies', 'Music']\n",
    "var_personal_feat_types = ['Finance', 'Phobias']\n",
    "fixed_feat_types = ['Personality', 'Health']\n",
    "\n",
    "label_types = ['Demographic']\n",
    "\n",
    "variables_by_type = {\n",
    "    'Demographics': ['Age', 'Height', 'Weight', 'Number of siblings', \n",
    "                     'Gender', 'Hand', 'Education', 'Only child', 'Home Town Type',\n",
    "                     'Home Type'],\n",
    "    'Finance': ['Finances', 'Shopping centres', 'Branded clothing', \n",
    "                'Entertainment spending', 'Spending on looks', \n",
    "                'Spending on gadgets', 'Spending on healthy eating'],\n",
    "    'Health': ['Smoking', 'Alcohol', 'Healthy eating'],\n",
    "    'Interests': ['History', 'Psychology', 'Politics', 'Mathematics', \n",
    "                  'Physics', 'Internet', 'PC', 'Economy Management', \n",
    "                  'Biology', 'Chemistry', 'Reading', 'Geography', \n",
    "                  'Foreign languages', 'Medicine', 'Law', 'Cars', \n",
    "                  'Art exhibitions', 'Religion', 'Countryside, outdoors', \n",
    "                  'Dancing', 'Musical instruments', 'Writing', 'Passive sport', \n",
    "                  'Active sport', 'Gardening', 'Celebrities', 'Shopping', \n",
    "                  'Science and technology', 'Theatre', 'Fun with friends', \n",
    "                  'Adrenaline sports', 'Pets'],\n",
    "    'Movies': ['Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', \n",
    "               'Sci-fi', 'War', 'Fantasy/Fairy tales', 'Animated', \n",
    "               'Documentary', 'Western', 'Action'],\n",
    "    'Music': ['Music', 'Slow songs or fast songs', 'Dance', 'Folk', \n",
    "              'Country', 'Classical music', 'Musical', 'Pop', 'Rock', \n",
    "              'Metal or Hardrock', 'Punk', 'Hiphop, Rap', 'Reggae, Ska', \n",
    "              'Swing, Jazz', 'Rock n roll', 'Alternative', 'Latino', \n",
    "              'Techno, Trance', 'Opera'],\n",
    "    'Personality': ['Daily events', 'Prioritising workload', \n",
    "                    'Writing notes', 'Workaholism', 'Thinking ahead', \n",
    "                    'Final judgement', 'Reliability', 'Keeping promises', \n",
    "                    'Loss of interest', 'Friends versus money', 'Funniness', \n",
    "                    'Fake', 'Criminal damage', 'Decision making', 'Elections', \n",
    "                    'Self-criticism', 'Judgment calls', 'Hypochondria', \n",
    "                    'Empathy', 'Eating to survive', 'Giving', \n",
    "                    'Compassion to animals', 'Borrowed stuff', \n",
    "                    'Loneliness', 'Cheating in school', 'Health', \n",
    "                    'Changing the past', 'God', 'Dreams', 'Charity', \n",
    "                    'Number of friends', 'Punctuality', 'Lying', 'Waiting', \n",
    "                    'New environment', 'Mood swings', 'Appearence and gestures', \n",
    "                    'Socializing', 'Achievements', 'Responding to a serious letter', \n",
    "                    'Children', 'Assertiveness', 'Getting angry', \n",
    "                    'Knowing the right people', 'Public speaking', \n",
    "                    'Unpopularity', 'Life struggles', 'Happiness in life', \n",
    "                    'Energy levels', 'Small - big dogs', 'Personality', \n",
    "                    'Finding lost valuables', 'Getting up', 'Interests or hobbies', \n",
    "                    \"Parents' advice\", 'Questionnaires or polls', 'Internet usage'],\n",
    "    'Phobias': ['Flying', 'Storm', 'Darkness', 'Heights', 'Spiders', 'Snakes', \n",
    "                'Rats', 'Ageing', 'Dangerous dogs', 'Fear of public speaking']\n",
    "}\n",
    "\n",
    "labels = variables_by_type['Demographics']\n",
    "\n",
    "try:\n",
    "    random_seed = min([StudentID1, StudentID2])\n",
    "except NameError:\n",
    "    random_seed = StudentID1\n",
    "\n",
    "def which_featgroups():\n",
    "    np.random.seed(random_seed)\n",
    "    these_entertainments = np.random.choice(var_entertainment_feat_types, 2, replace=False).tolist()\n",
    "    these_personal = np.random.choice(var_personal_feat_types, 1, replace=False).tolist()\n",
    "    these_types = fixed_feat_types + these_personal + these_entertainments\n",
    "    print('*** THESE ARE THE SELECTED TYPE OF VARIABLES:')\n",
    "    for k in these_types:\n",
    "        print(f'{k}')\n",
    "    print('*************************************')\n",
    "    return these_types\n",
    "\n",
    "def which_features(these_types):\n",
    "    np.random.seed(random_seed)\n",
    "    these_features = []\n",
    "    for type in these_types:\n",
    "        if type != 'Personality':\n",
    "            these_features += variables_by_type[type]\n",
    "        else:\n",
    "            these_features += np.random.choice(variables_by_type[type], \n",
    "                                               int(2 * (len(variables_by_type[type]) / 3)), \n",
    "                                               replace=False).tolist()\n",
    "    print('*** THESE ARE THE SELECTED FEATURES:')\n",
    "    for ft in these_features:\n",
    "        print(f'{ft}')\n",
    "    print('*************************************')\n",
    "    return these_features\n",
    "\n",
    "these_types = which_featgroups()\n",
    "these_features = which_features(these_types)\n",
    "\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c9090-6065-4f25-bdc3-1b3cdad6c083",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "In the following cell, import all the modules you think are necessary for doing the homework, **among the ones listed and used during the laboratories of the course**.\n",
    "No extra modules are allowed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad703d-e6d8-4239-8222-7aa91fbda3e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:13:58.155889Z",
     "start_time": "2024-12-21T14:13:58.144791Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT IMPORT NUMPY\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "import yaml\n",
    "from IPython.display import display  # to display variables in a \"nice\" way\n",
    "\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1143d0-e6a8-43dd-8bf8-0363842bac60",
   "metadata": {},
   "source": [
    "## Exercise 1. Preparing the Dataset\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. load the dataset \"_responses_hw.csv_\";\n",
    "2. create a working dataframe extracting from _responses_hw.csv_ the columns corresponding to the variables in _these_features_, and randomly selecting 2/3 of the rows. Let us call this dataframe _X_df_;\n",
    "3. analyze the obtained dataframe and performing cleansing/encoding operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6311d7",
   "metadata": {},
   "source": [
    "### Section 1.1 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2d477-033c-40fd-b31e-f7c03ddb2954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:32:23.314771Z",
     "start_time": "2024-12-21T14:32:22.489809Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://raw.githubusercontent.com/fabiolauro/PCA_CLA_Project/refs/heads/main/responses_hw.csv\"\n",
    "\n",
    "responses_df = pd.read_csv(url)\n",
    "rows = responses_df.shape[0]\n",
    "responses_df_rows = responses_df.sample(round(2*rows/3), replace = False, random_state= random_seed)\n",
    "responses_df_rows = responses_df_rows.sort_index()\n",
    "\n",
    "#now let's take only the columns we are interested in\n",
    "\n",
    "X_df_raw = responses_df_rows[these_features]\n",
    "\n",
    "display(X_df_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7171614b",
   "metadata": {},
   "source": [
    "### Section 1.2 Handling categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use dictionaries to map categorical variables into numerical values\n",
    "\n",
    "\n",
    "cat_variables = X_df_raw.select_dtypes(include=['object', 'category']).columns\n",
    "print(cat_variables)\n",
    "\n",
    "smoking = {\n",
    "    \"never smoked\": 1,\n",
    "    \"tried smoking\": 2,\n",
    "    \"former smoker\": 3,\n",
    "    \"current smoker\":4\n",
    "}\n",
    "\n",
    "drinking = {\n",
    "    \"never\": 1,\n",
    "    \"social drinker\": 2,\n",
    "    \"drink a lot\": 3\n",
    "}\n",
    "#Puntualità andrebbe chiamata Ritardatarietà affinche abbia senso nelle PC Main features in quanto è alta, ma un valore alto significa alto ritardo, perciò la scala è invertita per coerenza.\n",
    "punctuality = {\n",
    "    'early': 3,\n",
    "    \"on time\": 2,\n",
    "    \"late\": 1\n",
    "}\n",
    "\n",
    "internet = {\n",
    "    'no time at all': 1,\n",
    "    'less than an hour a day': 2,\n",
    "    'few hours a day': 3,\n",
    "    'most of the day': 4\n",
    "}\n",
    "\n",
    "X_df = X_df_raw.copy()\n",
    "\n",
    "X_df['Internet usage'] = X_df['Internet usage'].map(internet)\n",
    "X_df['Punctuality'] = X_df['Punctuality'].map(punctuality)\n",
    "X_df['Smoking'] = X_df['Smoking'].map(smoking)\n",
    "X_df['Alcohol'] = X_df['Alcohol'].map(drinking)\n",
    "\n",
    "#let's show that this works\n",
    "\n",
    "print(X_df_raw[\"Alcohol\"], X_df[\"Alcohol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee360f0f",
   "metadata": {},
   "source": [
    "### Section 1.3 Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8af6b",
   "metadata": {},
   "source": [
    "We looked for **missing values** in the sampled DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d68d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for missing values, cleaning operations\n",
    "\n",
    "missing_values = X_df.isnull().sum()\n",
    "missing_values = missing_values.sort_values(ascending=False)\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2256a88",
   "metadata": {},
   "source": [
    "Dropping all the Rows with missing values means discarding 1/3 of the rows, so we looked for some pattern in the missingness of the data plotting the dataset as a Heatmap, highlighting the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(X_df.isnull(), aspect='auto', cmap='viridis', interpolation='none')\n",
    "plt.colorbar(label='Missing (1=True, 0=False)')\n",
    "plt.title(\"Heatmap of Missing Values\")\n",
    "plt.xlabel(\"Columns - Features\")\n",
    "plt.ylabel(\"Rows -  Entries\")\n",
    "#plt.xticks(range(X_df.shape[1]), X_df.columns, rotation=45)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f4b00",
   "metadata": {},
   "source": [
    "The data appears to align closely with the Missing Completely At Random (MCAR) assumption. This occurs when the likelihood of missing data is uniformly distributed across all observations, without discernible patterns or dependencies on observed or unobserved variables.\n",
    "\n",
    "In our analysis:\n",
    "\n",
    "- We found no observable patterns or relationships explaining the missing data.\n",
    "- There was no specific column or feature that could reliably guide selective row preservation when removing incomplete records.\n",
    "- Imputation of missing values, while a potential approach, would introduce synthetic information, which could bias the results without strong justification or domain-specific knowledge to support the method chosen.\n",
    "\n",
    "Given these considerations, we opted to discard rows with missing values to ensure the integrity of the analysis, avoiding the risks associated with introducing artificial or potentially misleading information.\n",
    "\n",
    "it's preferable to have less data but correct philosophy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#if we drop all the missing values, we lose about 1/3 of the rows\n",
    "#we could only drop values for columns with more than a given threshold of missing values, for example 5\n",
    "\n",
    "#missing_values = missing_values[missing_values > 3]\n",
    "\n",
    "#print(missing_values)\n",
    "\n",
    "\n",
    "#how_many = np.sum(missing_values)\n",
    "#print(how_many)\n",
    "#it shows that the missing values corresponding to variables with more than 3 missing values are 162, out of 281.\n",
    "#We could consider to fill those values, which won't affect sensibly the variance in the data since we are filling just a few values per variable.\n",
    "#We can talk about it with the professor. For now we see the results obtained by dropping all the missing values and\n",
    "#eventually just fix this part of the code\n",
    "\n",
    "\n",
    "#removing missing values\n",
    "\n",
    "#for col in X_df.columns:\n",
    "#    mode_value = X_df[col].mode()[0]  # .mode() restituisce una Series; prendiamo il primo valore\n",
    "#    X_df[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "X_df = X_df.dropna()\n",
    "\n",
    "display(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e5aa4",
   "metadata": {},
   "source": [
    "### Section 1.3 Defining the final DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:57:38.048552Z",
     "start_time": "2024-12-21T14:57:37.952133Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now that we have cleaned the dataset, let's save a filtered version of the original one\n",
    "#which contains also the demographic variables, that will be useful later\n",
    "\n",
    "ind_filt = X_df.index\n",
    "resp_filt = responses_df_rows.loc[ind_filt,:]\n",
    "display(resp_filt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4058cb9-19fc-406c-8380-7e8053b43649",
   "metadata": {},
   "source": [
    "## Exercise 2. Analyzing the Variance and the PCs\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. create two new dataframes from _X_df_ applying a StandardScaler and a MinMaxscaler. Call these new dataframes as _Xstd_df_ and _Xmm_df_, respectively;\n",
    "2. compute the variance of all the features in _X_df_, _Xstd_df_, and _Xmm_df_ and **comment the results**;\n",
    "3. compute all the $n$ Principal Components (PCs) for each dataset _X_df_, _Xstd_df_, and _Xmm_df_. Then, visualize the curves of the cumulative explained variances and **comment the results**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 2.1: Scaling the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "181358a6-6bb0-4f22-b395-34e3757fa16e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:33:23.266765Z",
     "start_time": "2024-12-21T14:33:23.220152Z"
    }
   },
   "outputs": [],
   "source": [
    "#transforming the datasets\n",
    "\n",
    "std = StandardScaler()\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "std.fit(X_df)\n",
    "minmax.fit(X_df)\n",
    "\n",
    "Xstd = std.transform(X_df)\n",
    "Xstd_df = pd.DataFrame(Xstd, columns= X_df.columns)\n",
    "\n",
    "Xmm = minmax.transform(X_df)\n",
    "Xmm_df = pd.DataFrame(Xmm, columns = X_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T16:52:14.102394Z",
     "start_time": "2024-12-21T16:52:14.082104Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(Xstd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T16:52:23.129815Z",
     "start_time": "2024-12-21T16:52:23.068636Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(Xmm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 2.2: Computing the variance of the three dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:33:20.477342Z",
     "start_time": "2024-12-21T14:33:19.781615Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#computing the variance\n",
    "var_x = X_df.var()\n",
    "var_std = Xstd_df.var()\n",
    "var_mm = Xmm_df.var()\n",
    "\n",
    "\n",
    "#plotting the variance of all the datasets\n",
    "fig_var, ax_var = plt.subplots(1,3, figsize=(16,6))\n",
    "\n",
    "ax_var[0].plot(var_x.values)\n",
    "ax_var[0].set_xlabel(\"Variables\")\n",
    "ax_var[0].set_ylabel(\"Variance\")\n",
    "ax_var[0].set_title(\"Variance of the original dataset\")\n",
    "ax_var[0].grid(True)\n",
    "\n",
    "ax_var[1].plot(var_std.values.round(4), color = \"green\")\n",
    "ax_var[1].set_xlabel(\"Variables\")\n",
    "ax_var[1].set_ylabel(\"Variance\")\n",
    "ax_var[1].set_title(\"Variance of the standardized dataset\")\n",
    "ax_var[1].grid(True)\n",
    "\n",
    "ax_var[2].plot(var_mm.values, color = \"red\")\n",
    "ax_var[2].set_xlabel(\"Variables\")\n",
    "ax_var[2].set_ylabel(\"Variance\")\n",
    "ax_var[2].set_title(\"Variance of the MinMax dataset\")\n",
    "ax_var[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- The original dataset presents a noticing variability in the variance values, where we go from a maximum of 2.5 to a minimum of 0.2 approximately. This range is not enormous but still significant.\n",
    "- The variance of the standardized dataset is equal to 1 for each variable, as expected due to the Standard Scaler functioning that subtracts the mean and divides by the standard deviation each value.\n",
    "- The MinMax dataset presents a similar behavior to the original one, in that the variables maintain their relative differences of the variance, although the range is much smaller in this case due to the rescaling: since each value is rescaled to fit into the interval of minimum and maximum for each variable, its absolute value is decreased, and this consequently reduces the absolute value of the variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 2.3: Computing the full PCA\n",
    "In this section we perform PCA without constraints on the number of components. We then compute the explained cumulative variance with respect to the PCs, and comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:33:31.065946Z",
     "start_time": "2024-12-21T14:33:26.630351Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std = PCA()\n",
    "pca_mm = PCA()\n",
    "\n",
    "pca_std.fit(Xstd_df)\n",
    "pca_mm.fit(Xmm_df)\n",
    "\n",
    "cumsum_std = np.cumsum(pca_std.explained_variance_ratio_)\n",
    "cumsum_mm = np.cumsum(pca_mm.explained_variance_ratio_)\n",
    "\n",
    "# Indici per il 33% e il 50% della varianza\n",
    "thresholds = [0.33, 0.50]\n",
    "indices_std = [np.argmax(cumsum_std >= t) for t in thresholds]\n",
    "indices_mm = [np.argmax(cumsum_mm >= t) for t in thresholds]\n",
    "\n",
    "fig_full, ax_full = plt.subplots(1,2, figsize = (14,6))\n",
    "ax_full[0].plot(cumsum_std)\n",
    "ax_full[0].hlines(y=0.33, xmin=0, xmax=indices_std[0])\n",
    "ax_full[0].hlines(y=0.50, xmin=0, xmax=indices_std[1])\n",
    "ax_full[0].vlines(x=indices_std[0], ymin=0, ymax=0.33)\n",
    "ax_full[0].vlines(x=indices_std[1], ymin=0, ymax=0.50)\n",
    "ax_full[0].text(indices_std[0], -0.03, f\" #PCs:{indices_std[0]}\", va='center', ha='center', color='blue')\n",
    "ax_full[0].text(indices_std[1], -0.03, f\" #PCs:{indices_std[1]}\", va='center', ha='center', color='blue')\n",
    "ax_full[0].set_xlabel(\"PCs\")\n",
    "ax_full[0].set_ylabel(\"% of variance\")\n",
    "ax_full[0].set_title(\"Explained variance cumulative sum - STD\")\n",
    "ax_full[0].grid(True)\n",
    "\n",
    "ax_full[1].plot(cumsum_mm, color = \"red\")\n",
    "ax_full[1].hlines(y=0.33, xmin=0, xmax=indices_mm[0])\n",
    "ax_full[1].hlines(y=0.50, xmin=0, xmax=indices_mm[1])\n",
    "ax_full[1].vlines(x=indices_mm[0], ymin=0, ymax=0.33)\n",
    "ax_full[1].vlines(x=indices_mm[1], ymin=0, ymax=0.50)\n",
    "ax_full[1].text(indices_mm[0], -0.03, f\"#PCs:{indices_mm[0]}\", va='center', ha='center', color='blue')\n",
    "ax_full[1].text(indices_mm[1], -0.03, f\"  #PCs:{indices_mm[1]}\", va='center', ha='center', color='blue')\n",
    "ax_full[1].set_xlabel(\"PCs\")\n",
    "ax_full[1].set_ylabel(\"% of variance\")\n",
    "ax_full[1].set_title(\"Explained variance cumulative sum - MinMax\")\n",
    "ax_full[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- We observe an almost identical behavior in the two PCA analyses. Both scaling methods require the same number of PCs to reach 33% of the variance, but MinMaxScaler condenses 50% of the variance into the first 17 PCs, compared to StandardScaler, which requires 19 PCs to achieve the same amount of explained variance.\n",
    "\n",
    "- In both representations, it was not possible to express 33% of the variance in fewer than 5 PCs, indicating that the problem is not only high-dimensional but also that the variance is spread across multiple dimensions.\n",
    "\n",
    "- This behavior aligns with theoretical expectations. StandardScaler tends to spread the variance more evenly among the various components, making it harder to have a few PCs capturing most of the variance. In contrast, MinMaxScaler preserves the variance allocation, allowing for a more efficient condensation into the first few components.\n",
    "\n",
    "- Theoretically, we expect the MinMax-scaled data to perform better in our analysis since the data is distributed as ranks (mostly within the same range, e.g., 1 to 5). On the other hand, StandardScaler is more effective in scenarios where the scales of the variables differ significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1771a-09b2-41bc-bc4c-7e3f6f530021",
   "metadata": {},
   "source": [
    "## Exercise 3. Dimensionality Reduction and PC Interpretation\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two dataframes _Xstd_df_, and _Xmm_df_, compute a new PCA for performing a dimensionality reduction with respect to $m$ dimensions. The value of $m$ must be $$m = \\min\\{m', 5\\}\\,,$$ where $m'$ is the value required for obtaining $33\\%$ of the total variance.\n",
    "2. For both the cases, visualize all the PCs and give a name/interpretation to them. **Comment and motivate your interpretations**. If possible, **compare the differences among the results obtained** for _Xstd_df_ and _Xmm_df_.\n",
    "3. Perform the score graph for both the cases (_std_ and _mm_). If $m>3$, plot the score graph with respect to the first 3 PCs. All the **plots must show the names of the PCs on the axes** for better understanding the results.\n",
    "4. **Optional:** plot more score graphs, coloring the dots with respect to any label in the list _labels_ that you believe can be interesting. **Comment and analyze this optional plots**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6cc17",
   "metadata": {},
   "source": [
    "### Section 3.1 Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c6c6d02-e58c-4db4-8872-d45c6eb9e4f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:33:32.693303Z",
     "start_time": "2024-12-21T14:33:32.670409Z"
    }
   },
   "outputs": [],
   "source": [
    "#we define a function to compute the PCA\n",
    "m = 5\n",
    "\n",
    "def pca_computation(df,max):\n",
    "\n",
    "    print(f\"Computing the PCA for the dataframe -- the maximum number of components is {max}\")\n",
    "\n",
    "    for i in range(1,max+1):\n",
    "        pca = PCA(n_components=i)\n",
    "        pca.fit(df)\n",
    "        exp_var = np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "        if exp_var >= 0.33:\n",
    "            print(f\"PCA has n={i} components!\")\n",
    "            print(f\"Variance explained: {np.sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "            fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "            ax[0].bar(range(pca.n_components_), pca.explained_variance_ratio_ ,color = \"skyblue\", edgecolor = \"black\")\n",
    "            ax[0].set_xticks(range(pca.n_components_), [f\"PC{k}\" for k in range(1,pca.n_components_+1)])\n",
    "            ax[0].set_xlabel(\"Principal Components\")\n",
    "            ax[0].set_ylabel(\"Variance percentage per PC\")\n",
    "            ax[0].grid(True)\n",
    "\n",
    "            ax[1].bar(range(pca.n_components_), np.cumsum(pca.explained_variance_ratio_), color = \"red\", edgecolor = \"black\")\n",
    "            ax[1].set_xticks(range(pca.n_components_), [f\"PC{k}\" for k in range(1,pca.n_components_+1)])\n",
    "            ax[1].set_xlabel(\"Principal Components\")\n",
    "            ax[1].set_ylabel(\"Cumulative variance\")\n",
    "            ax[1].grid(True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            return pca\n",
    "    print(\"Couldn't find a PCA explaining at least 33% of the variance. 5 components are given\")\n",
    "    print(f\"Variance explained: {np.sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "\n",
    "    ax[0].bar(range(pca.n_components_), pca.explained_variance_ratio_ ,color = \"skyblue\", edgecolor = \"black\")\n",
    "    ax[0].set_xticks(range(pca.n_components_), [f\"PC{k}\" for k in range(1,pca.n_components_+1)])\n",
    "    ax[0].set_xlabel(\"Principal Components\")\n",
    "    ax[0].set_ylabel(\"Variance percentage per PC\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].bar(range(pca.n_components_), np.cumsum(pca.explained_variance_ratio_), color = \"red\", edgecolor = \"black\")\n",
    "    ax[1].set_xticks(range(pca.n_components_), [f\"PC{k}\" for k in range(1,pca.n_components_+1)])\n",
    "    ax[1].set_xlabel(\"Principal Components\")\n",
    "    ax[1].set_ylabel(\"Cumulative variance\")\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return pca\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 3.2 PCA over Standardized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T16:51:05.077615Z",
     "start_time": "2024-12-21T16:51:04.247971Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std_m = pca_computation(Xstd_df,m) #computation of the PCA for Xstd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It was not possible to find a less than 5 PC capable to explain at least 33% of the Variance so the first 5 PC will be analized.\n",
    "\n",
    "\n",
    "Since we have approximately 100 features, for each principal component, we will consider  $eps$ : PER FABIO - SPIEGA COME INTENDEVI USARLO, usalo pure, tanto le feature piu importanti rimangono le prime e le interpretazioni che abbiamo date sono coerenti, se hai problemi con i grafici scriviamo che riportiamo meno feature del dovuto nel grafico.\n",
    "\n",
    "$$\n",
    "\\text{eps} = \\sqrt{\\frac{1}{X_{\\text{df}}.\\text{shape}[1]}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's define the threshold to identify the features relevant to each principal component\n",
    "eps = np.sqrt(1/X_df.shape[1])\n",
    "print(eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:33:52.820282Z",
     "start_time": "2024-12-21T14:33:52.805094Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here we assign a color to each feature based on its type\n",
    "set3 = cm.Set3.colors\n",
    "\n",
    "color_by_type = {\n",
    "\n",
    "\n",
    "    \n",
    "    'Health' : set3[0],\n",
    "    'Interests' : set3[1],\n",
    "    'Music' : set3[2],\n",
    "    'Personality' : set3[3],\n",
    "    'Phobias' : set3[4]\n",
    "}\n",
    "\n",
    "variable_to_color = {}\n",
    "\n",
    "for var_type, variables in variables_by_type.items():\n",
    "    if var_type in color_by_type:\n",
    "        color = color_by_type[var_type]\n",
    "        for variable in variables:\n",
    "            variable_to_color[variable] = color\n",
    "\n",
    "\n",
    "#we also define a legend for the plots\n",
    "\n",
    "type_colors_legend = [Line2D([0], [0], color=color_by_type[k], label = k) for k in color_by_type]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:33:58.100259Z",
     "start_time": "2024-12-21T14:33:55.743015Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_components_std = pca_std_m.n_components_ #number of components\n",
    "\n",
    "#we define a few lists that will be useful to store all the relevant quantities\n",
    "\n",
    "loadings_std = [] #loadings of the components\n",
    "indexes_std = [] #indexes of the most important features for each component\n",
    "top_loadings_std = [] #top loadings\n",
    "top_features_std = [] #corresponding names of the top features\n",
    "features_number = 15\n",
    "\n",
    "#in this cycle we find the most important features for each component, then we plot them\n",
    "\n",
    "for ii in range(n_components_std):\n",
    "\n",
    "    loadings = pca_std_m.components_[ii,:]\n",
    "    indexes = np.argsort(np.abs(loadings))[-features_number:]\n",
    "    top_loadings = loadings[indexes]\n",
    "    top_features = X_df.columns[indexes]\n",
    "    top_colors = [variable_to_color[var] for var in top_features]\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.title(\"Expl.Var.Ratio: \"+str(pca_std_m.explained_variance_ratio_[ii]))\n",
    "    plt.bar(range(features_number),  top_loadings, color = top_colors, width=0.2, edgecolor = \"black\")\n",
    "    plt.xticks(range(features_number), labels= top_features, fontsize=6)\n",
    "    plt.legend(handles = type_colors_legend, title = \"Question type\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    loadings_std.append(loadings)\n",
    "    indexes_std.append(indexes)\n",
    "    top_loadings_std.append(top_loadings)\n",
    "    top_features_std.append(top_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### ****Principal Component Analysis (PCA) - STD Dataset****\n",
    "\n",
    "---\n",
    "\n",
    "#### **PC1: \"Cultural and Artistic Interests\"**\n",
    "\n",
    "- **Main Features**:  \n",
    "  - The features with a high contribution to the first Principal Component are **opera**, **classical music**, **theater**, and **jazz**, which indicate a particular focus on **Performing Arts**. Along with **reading**, **Latin**, and other cultural indicators, they define a person's interest in artistic and cultural subjects.\n",
    "\n",
    "- **Interpretation**:  \n",
    "  - This axis differentiates individuals with a strong interest in arts and culture from those less engaged in these areas.\n",
    "\n",
    "---\n",
    "\n",
    "#### **PC2: \"Rational Self-Control vs Emotional Sensibility\"**\n",
    "\n",
    "- **Caratteristiche principali**:\n",
    "\n",
    "    - **Positive Contributions**:\n",
    "        - Derived from scientific and rational subjects like **physics** and **technology**.\n",
    "        - Associated with bold music genres such as **heavy metal** and **rock**, which express a defiance of fears.\n",
    "\n",
    "    - **Negative Contributions**:\n",
    "        - Linked to common and specific fears, such as fear of **darkness**, **spiders**, or **snakes**.\n",
    "        - Contrasted with milder musical interests like **pop music**, which is less bold and expressive.\n",
    "\n",
    "- **Interpretation**:\n",
    "    - This component reflects a spectrum between \"Fearless and Rational Individuals\" and \"Vulnerable and Emotionally Reactive Individuals\".\n",
    "\n",
    "---\n",
    "\n",
    "#### **PC3: \"Extroversion vs Introversion\"**\n",
    "\n",
    "- **Positive Contributions**:\n",
    "        - Associated with sociability and an active lifestyle, involving **sports**, **socializing**, and **fun with friends**, emphasizes extroversion social dynamism.\n",
    "\n",
    "- **Negative Contributions**:\n",
    "      - They express feelings of **loneliness** and shyness in unfamiliar settings, along with a **fear of public speaking**, while showing a preference for less dynamic or more solitary activities, such as **reading**.\n",
    "\n",
    "- **Interpretation**:\n",
    "    - This component contrasts Socially Active and Extroverted Individuals with Introverted and Shy ones.\n",
    "---\n",
    "\n",
    "#### **PC4: \"Bad habits vs Analytical Focus\"**\n",
    "\n",
    "- **Positive Contributions**:\n",
    "    - Linked to behaviors such as **drinking alcohol**, **smoking cigarettes**, and **cheating in school**, which reflect a tendency towards risk-taking, a lack of focus, and challenges with self-discipline or commitment.\n",
    "\n",
    "- **Negative Contributions**:\n",
    "    - Associated with logical and analytical subjects like **math**, **physics**, and **science**, showing a stronger focus on problem-solving and structured thinking, with less interest in recreational or social activities.\n",
    "\n",
    "- **Interpretation**:\n",
    "    - This component contrasts individuals who are more emotionally driven and prone to risky behaviors with those who are more analytical, disciplined, and goal-oriented.\n",
    "---\n",
    "\n",
    "#### **PC5: \"Social Integration vs Marginalization\"**\n",
    "\n",
    "- **Positive Contributions**:\n",
    "    - Associated with traits that promote social integration, such as **happiness**, **self-confidence**, and **fun**. \n",
    "    - Includes participation in **recreational activities** like **sports**, **going to the gym**, and enjoying social interactions, indicating an active and socially connected lifestyle.\n",
    "\n",
    "- **Negative Contributions**:\n",
    "    - Reflects elements of marginalization, such as **hypochondria**, **fear of public speaking**, and **eating disorders**, which may lead to social withdrawal or difficulty in building connections.\n",
    "    - Also tied to insecurities and emotional struggles that hinder meaningful participation in social settings.\n",
    "\n",
    "- **Interpretation**:\n",
    "    - This component highlights a spectrum between individuals who are socially integrated, confident, and engaged versus those who face emotional challenges and risks of social exclusion or marginalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:34:04.245450Z",
     "start_time": "2024-12-21T14:34:04.225220Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can define the PCA features as follows:\n",
    "\n",
    "pc_std_names = [\"Arts and Cultural interests\", \"Rational Self-Control vs Impressionable\", \"Extroversion vs Introversion\", \"Bad habits vs Science Inclination\", \"Integrated vs Marginalized\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 3.3 PCA over MaxMin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:34:08.023155Z",
     "start_time": "2024-12-21T14:34:05.880192Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_minmax_m = pca_computation(Xmm_df,m) #computation of the PCA for Xmm_df\n",
    "\n",
    "n_components_mm = pca_minmax_m.n_components_\n",
    "\n",
    "loadings_mm = [] #loadings\n",
    "indexes_mm = [] #list to store the indexes of the top 10 loadings\n",
    "top_loadings_mm = [] #top loadings\n",
    "top_features_mm = [] #names of the corresponding top 10 features\n",
    "n_features = 30\n",
    "\n",
    "for ii in range(n_components_mm):\n",
    "\n",
    "    loadings = pca_minmax_m.components_[ii,:]\n",
    "    indexes = np.argsort(np.abs(loadings))[-n_features:]\n",
    "    top_loadings = loadings[indexes]\n",
    "    top_features = X_df.columns[indexes]\n",
    "    top_colors = [variable_to_color[var] for var in top_features]\n",
    "\n",
    "    plt.figure(figsize=(28,6))\n",
    "    plt.title(\"Expl.Var.Ratio: \"+str(pca_minmax_m.explained_variance_ratio_[ii]))\n",
    "    plt.bar(range(n_features), top_loadings, color = top_colors, width = 0.2, edgecolor = \"black\")\n",
    "    plt.xticks(range(n_features), fontsize = 8, labels= top_features)\n",
    "    plt.legend(handles = type_colors_legend, title = \"Question type\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    loadings_mm.append(loadings)\n",
    "    indexes_mm.append(indexes)\n",
    "    top_loadings_mm.append(top_loadings)\n",
    "    top_features_mm.append(top_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e17e4b",
   "metadata": {},
   "source": [
    "#### ****Principal Component Analysis (PCA) - MinMax Dataset **** \n",
    "\n",
    "---\n",
    "-   N.B. In naming the PCs that presents both, negative and positive influence from features, the name that comes first is alwais associated to the intrepretation of the positive direction. ES: Extrovert vs Introvert - The features with positive loadings on the PC where those that leed us to the association of extrovertion to the increasing direction of the component. This convention is respected in the whole report and helps understanding results and visualizations.\n",
    "---\n",
    "#### **PC1: \"Cultural and Artistic Interests\"**\n",
    "\n",
    "- **Main Features**:  \n",
    "  - The features with a high contribution to the first Principal Component are **opera**, **classical music**, **theater**, and **jazz**, which indicate a particular focus on **Performing Arts**. Along with **reading**, **Latin**, and other cultural indicators, they define a person's interest in artistic and cultural subjects.\n",
    "\n",
    "- **Interpretation**:  \n",
    "  - This axis differentiates individuals with a strong interest in arts and culture from those less engaged in these areas.\n",
    "\n",
    "---\n",
    "\n",
    "#### **PC2: \"Rational and Alternative vs Impressionable and Emotional\"**\n",
    "\n",
    "- **Caratteristiche principali**:\n",
    "\n",
    "    - **Positive Contributions**:\n",
    "        - Linked to common and specific fears, such as fear of **darkness**, **spiders**, or **snakes**.\n",
    "        - Shows interest for mainstream and pop culture since attributes like **Celebrities** results influent.\n",
    "\n",
    "    - **Negative Contributions**:\n",
    "        - Derived from scientific and rational subjects like **physics** and **technology**.\n",
    "        - Associated with alternative music genres such as **heavy metal** and **alternative** life style.\n",
    "\n",
    "- **Interpretation**:\n",
    "    - This component reflects a spectrum between \"Alternative and Rational\" and \" Emotionally Reactive and Impressionable\" individuals.\n",
    "\n",
    "---\n",
    "\n",
    "#### **PC3: \"Energic and Proactive Individuals\"**\n",
    "\n",
    "- **Main Feature**:\n",
    "    - The most contributing features involve **adrenaline sports** and **active sports**, together with interests in **cars**, **hip-hop music**, and various **socio-economic** activity indicators.\n",
    "\n",
    "- **Interpretation**:\n",
    "    - This component represents a dimension along which individuals are distributed based on their **proactive behavior** toward society, and social activities specially when those are Adrenalinic and Active sports.\n",
    "\n",
    "#### **PC4: \"Bad habits vs Analytical Focus\"**\n",
    "\n",
    "- **Positive Contributions**:\n",
    "    - Associated with logical and analytical subjects like **Mathematics**, **Physics**, and **Science**, showing a stronger focus on problem-solving and structured thinking, with less interest in recreational or social activities.\n",
    "\n",
    "- **Negative Contributions**:\n",
    "    - Linked to behaviors such as **drinking alcohol**, **smoking cigarettes**, and **cheating in school**, which reflect a tendency towards risk-taking, a lack of focus, and challenges with self-discipline or commitment.\n",
    "\n",
    "\n",
    "- **Interpretation**:\n",
    "    - This component contrasts individuals who are more emotionally driven and prone to risky behaviors with those who are more analytical, disciplined, and goal-oriented.\n",
    "---\n",
    "\n",
    "#### **PC5: \"Politics vs Medical Sciences\"** ####\n",
    "\n",
    "- **Positive Contributions**:\n",
    "    -\n",
    "\n",
    "- **Negative Contributions**:\n",
    "    -\n",
    "\n",
    "- **Interpretation**: \n",
    "    -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:34:25.946077Z",
     "start_time": "2024-12-21T14:34:25.915736Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can give then each PC a name\n",
    "\n",
    "pc_mm_names = [\"Arts and Cultural interests\", \"Impressionable vs Rational / Alternative\", \"Energic and Proactive Individuals\", \"Science vs Bad habits\", \"Politics vs Medical Sciences\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### ****Comparison between the results from the two analisys**** ####\n",
    "As expected, the results from the two analyses don't differ much when looking at the first components! While the features are presented in a different order when ranked by their contribution to the Principal Components, the meaning of the main three PCs is almost identical, with some slight differences, such as the slightly remarked emphasis on music interests captured from the second component in the **Std** scaled data, that's missing in the **MinMax** scaled one, or the fact that the interpretation of the 3rd component in the **MinMax** leaded us to name it \"Energic and Proactive\", characteristics that when declined in social contests results in Extrovert individuals as the ones described from the 3rd PC of the **Std** . The fourth and fifth Principal Components start to show notable differences not only in terms of ordering but also in the presence of influential features, leading to a different interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Section 3.4: Score Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To plot the score graph, we first have to project the data in the new reference system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3456336",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mm = pca_minmax_m.transform(Xmm_df)\n",
    "Y_std = pca_std_m.transform(Xstd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now plot the score graph for the standardized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:43:34.052721Z",
     "start_time": "2024-12-21T14:43:33.999667Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(variables_by_type[\"Demographics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:58:26.385552Z",
     "start_time": "2024-12-21T14:58:26.355472Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We first define a color set for each label we are interested in.\n",
    "#We choose to study the labels related to age, gender, education, home town type\n",
    "set1 = cm.Set1.colors\n",
    "set2 = cm.Set2.colors\n",
    "\n",
    "\n",
    "edu = resp_filt['Education'].unique()\n",
    "print(f\"Education levels are: {list(edu)}\")\n",
    "\n",
    "home_town = resp_filt['Home Town Type'].unique()\n",
    "print(f\"Different home town types: {list(home_town)}\")\n",
    "\n",
    "age = resp_filt[\"Age\"].unique()\n",
    "print(f\"Different age levels: {age}\")\n",
    "\n",
    "gender = resp_filt[\"Gender\"].unique()\n",
    "print(f\"Genders represented: {list(gender)}\")\n",
    "\n",
    "hand = resp_filt[\"Hand\"].unique()\n",
    "print(f\"Hand: {list(hand)}\")\n",
    "\n",
    "OnlyChild = resp_filt[\"Only child\"].unique()\n",
    "print(f\"Only child: {list(OnlyChild)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:08:32.498720Z",
     "start_time": "2024-12-21T15:08:32.483031Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color_hand_dict = {\n",
    "    'right' : set1[0],\n",
    "    'left' : set1[1],\n",
    "    'missing' : 'grey'\n",
    "}\n",
    "\n",
    "color_onlychild_dict = {\n",
    "    'no' : set1[0],\n",
    "    'yes' : set1[1],\n",
    "    'missing' : 'grey'\n",
    "}\n",
    "\n",
    "color_education_dict = {\n",
    "    'doctorate degree' : (68/255, 1/255, 84/255),\n",
    "    'masters degree': (64/255, 67/255, 135/255),\n",
    "    'college/bachelor degree' : (41/255, 120/255, 142/255),\n",
    "    'secondary school' :  (34/255, 167/255, 132/255),\n",
    "    'primary school' : (121/255, 209/255, 81/255),\n",
    "    'currently a primary school pupil': (253/255, 231/255, 36/255),\n",
    "    'missing': 'grey'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "color_town_type_dict = {\n",
    "    'city' : set2[0],\n",
    "    'village': set2[1],\n",
    "    'missing': 'grey'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "color_gender_dict = {\n",
    "    'female': set2[3],\n",
    "    'male': set2[2],\n",
    "    'missing': 'grey'\n",
    "}\n",
    "\n",
    "color_age_dict = {\n",
    "    '15.0': (0.267, 0.005, 0.329),\n",
    "    '16.0': (0.267, 0.005, 0.329),\n",
    "    '17.0': (0.283, 0.105, 0.427),\n",
    "    '18.0': (0.275, 0.195, 0.496),\n",
    "    '19.0': (0.249, 0.279, 0.535),\n",
    "    '20.0': (0.212, 0.36, 0.552),\n",
    "    '21.0': (0.181, 0.43, 0.557),\n",
    "    '22.0': (0.153, 0.497, 0.558),\n",
    "    '23.0': (0.128, 0.567, 0.551),\n",
    "    '24.0': (0.122, 0.633, 0.53),\n",
    "    '25.0': (0.176, 0.698, 0.491),\n",
    "    '26.0': (0.289, 0.758, 0.428),\n",
    "    '27.0': (0.449, 0.814, 0.335),\n",
    "    '28.0': (0.627, 0.855, 0.223),\n",
    "    '29.0': (0.815, 0.883, 0.11),\n",
    "    '30.0': (0.993, 0.906, 0.144),\n",
    "    'missing':'grey'\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T15:08:34.637624Z",
     "start_time": "2024-12-21T15:08:34.608379Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we create the corresponding columns\n",
    "#we manage missing values with the grey color\n",
    "\n",
    "colors_edu = resp_filt[\"Education\"].apply(lambda x: color_education_dict[x] if pd.notnull(x) else color_education_dict['missing'])\n",
    "colors_town = resp_filt[\"Home Town Type\"].apply(lambda x: color_town_type_dict[x] if pd.notnull(x) else color_town_type_dict['missing'])\n",
    "colors_gen = resp_filt[\"Gender\"].apply(lambda x: color_gender_dict[x] if pd.notnull(x) else color_gender_dict['missing'])\n",
    "colors_age = resp_filt[\"Age\"].apply(lambda x: color_age_dict[str(x)] if pd.notnull(x) else color_age_dict['missing'])\n",
    "colors_onlychild = resp_filt[\"Only child\"].apply(lambda x: color_onlychild_dict[x] if pd.notnull(x) else color_onlychild_dict['missing'])\n",
    "colors_hand = resp_filt[\"Hand\"].apply(lambda x: color_hand_dict[x] if pd.notnull(x) else color_hand_dict['missing'])\n",
    "\n",
    "colors_edu_legend = [Line2D([0], [0], color=color_education_dict[k], label = k) for k in color_education_dict.keys()]\n",
    "colors_town_legend = [Line2D([0], [0], color=color_town_type_dict[k], label = k) for k in color_town_type_dict.keys()]\n",
    "colors_gen_legend = [Line2D([0], [0], color=color_gender_dict[k], label=k) for k in color_gender_dict.keys()]\n",
    "colors_age_legend = [Line2D([0], [0], color=color_age_dict[age], label=[age]) for age in color_age_dict.keys()]\n",
    "color_hand_legend = [Line2D([0], [0], color=color_hand_dict[age], label=[age]) for age in color_hand_dict.keys()]\n",
    "\n",
    "if 'missing' not in color_gender_dict:\n",
    "    colors_age_legend.append(Line2D([0], [0], color='gray', label='missing'))\n",
    "\n",
    "colors_onlychild_legend = [Line2D([0], [0], color=color_onlychild_dict[k], label=k) for k in color_onlychild_dict.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Score graph 1.1: Xstd dataset, Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:24:43.509863Z",
     "start_time": "2024-12-21T17:24:43.232360Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Uncomment those lines togheter with the last one (input) to make scatterplot interactive\n",
    "\"\"\" %matplotlib widget\n",
    "plt.ion() \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "sg_std1 = plt.figure(figsize=(12, 12))\n",
    "ax_sg_std1 = sg_std1.add_subplot(111, projection='3d')\n",
    "ax_sg_std1.scatter(Y_std[:,0],Y_std[:,1],Y_std[:,2])\n",
    "#ax_sg_std1.scatter(Y_std[:,0],Y_std[:,1],Y_std[:,2], color= colors_gen) NON RICHIEDE DI COLORARE IL GRAFICO IN QUALCHE MODO\n",
    "plt.title('Standardized dataset - SCORE GRAPH')\n",
    "ax_sg_std1.set_xlabel(pc_std_names[0])\n",
    "ax_sg_std1.set_ylabel(pc_std_names[1])\n",
    "ax_sg_std1.set_zlabel(pc_std_names[2])\n",
    "plt.grid()\n",
    "#plt.switch_backend('tkagg')\n",
    "plt.show()\n",
    "\n",
    "\"\"\" input(\"Rotate or zoom the plot, then press Enter to exit.\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the PCA with 5 components does not even reach 33% of the variance in the data, it is reasonable that we don't see a clear trend or specific areas in the score graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's have a look at the first two dimensions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T16:33:57.523051Z",
     "start_time": "2024-12-21T16:33:57.330273Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attribute_tuples = [\n",
    "    ['Education', colors_edu, colors_edu_legend, color_education_dict],\n",
    "    ['Town', colors_town, colors_town_legend, color_town_type_dict],\n",
    "    ['Gender', colors_gen, colors_gen_legend, color_gender_dict],\n",
    "    ['Age', colors_age, colors_age_legend, color_age_dict],\n",
    "    ['Only Child', colors_onlychild, colors_onlychild_legend, color_onlychild_dict],\n",
    "    ['Hand', colors_hand, color_hand_legend, color_hand_dict]\n",
    "]\n",
    "\n",
    "\n",
    "for [label, color, legend, dict_attr] in attribute_tuples:\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6)) \n",
    "\n",
    "    axs[0].scatter(Y_std[:, 0], Y_std[:, 1], color=color)\n",
    "    axs[0].set_title(\"Standardized dataset-\" + label)\n",
    "    axs[0].set_xlabel(pc_std_names[0])  \n",
    "    axs[0].set_ylabel(pc_std_names[1])  \n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend(legend, [k for k in dict_attr.keys()])\n",
    "\n",
    "    axs[1].scatter(Y_std[:, 0], Y_std[:, 2], color=color)\n",
    "    axs[1].set_title(\"Standardized dataset-\" + label)\n",
    "    axs[1].set_xlabel(pc_std_names[0])  \n",
    "    axs[1].set_ylabel(pc_std_names[2])  \n",
    "    axs[1].grid(True)\n",
    "    axs[1].legend(legend, [k for k in dict_attr.keys()])\n",
    "\n",
    "    axs[2].scatter(Y_std[:, 1], Y_std[:, 2], color=color)\n",
    "    axs[2].set_title(\"Standardized dataset-\" + label)\n",
    "    axs[2].set_xlabel(pc_std_names[1])  \n",
    "    axs[2].set_ylabel(pc_std_names[2])  \n",
    "    axs[2].grid(True)\n",
    "    axs[2].legend(legend, [k for k in dict_attr.keys()])\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    " \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Score graph 2.1: MinMax dataset, education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:24:09.423372Z",
     "start_time": "2024-12-21T17:24:09.205432Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#education score graph wit MinMax dataset\n",
    "\n",
    "\n",
    "sg_mm1 = plt.figure(figsize=(12, 12))\n",
    "ax_sg_mm1 = sg_mm1.add_subplot(111, projection='3d')\n",
    "ax_sg_mm1.scatter(Y_mm[:,0],Y_mm[:,1],Y_mm[:,2])\n",
    "plt.title('MinMax dataset - SCORE GRAPH')\n",
    "ax_sg_mm1.set_xlabel(pc_std_names[0])\n",
    "ax_sg_mm1.set_ylabel(pc_std_names[1])\n",
    "ax_sg_mm1.set_zlabel(pc_std_names[2])\n",
    "plt.legend(colors_edu_legend, [k for k in color_education_dict.keys()])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T16:36:41.994124Z",
     "start_time": "2024-12-21T16:36:41.771255Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for [label, color, legend, dict_attr] in attribute_tuples:\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6)) \n",
    "\n",
    "    axs[0].scatter(Y_mm[:, 0], Y_mm[:, 1], color=color)\n",
    "    axs[0].set_title(\"MinMax dataset-\" + label)\n",
    "    axs[0].set_xlabel(pc_mm_names[0])  \n",
    "    axs[0].set_ylabel(pc_mm_names[1])  \n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend(legend, [k for k in dict_attr.keys()])\n",
    "\n",
    "    axs[1].scatter(Y_mm[:, 0], Y_mm[:, 2], color=color)\n",
    "    axs[1].set_title(\"MinMax dataset-\" + label)\n",
    "    axs[1].set_xlabel(pc_mm_names[0])  \n",
    "    axs[1].set_ylabel(pc_mm_names[2])  \n",
    "    axs[1].grid(True)\n",
    "    axs[1].legend(legend, [k for k in dict_attr.keys()])\n",
    "\n",
    "    axs[2].scatter(Y_mm[:, 1], Y_mm[:, 2], color=color)\n",
    "    axs[2].set_title(\"MinMax dataset-\" + label)\n",
    "    axs[2].set_xlabel(pc_mm_names[1])  \n",
    "    axs[2].set_ylabel(pc_mm_names[2])  \n",
    "    axs[2].grid(True)\n",
    "    axs[2].legend(legend, [k for k in dict_attr.keys()])\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    " \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8f0b8-37c5-403a-93ad-8abefd96e3b2",
   "metadata": {},
   "source": [
    "## Exercise 4. $k$-Means\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), run the $k$-Means for clustering the data. In particular, **use the silohuette score for identify the best value for $k\\in\\{3, \\ldots, 10\\}$**.\n",
    "2. Plot the score graphs of exercise 3.3, adding the centroids of the cluster.\n",
    "3. Observing the centroids coordinates in the PC space, **give a name/interpretation to them**, exploiting the names you assigned to the PCs. **Comment and motivate your interpretations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 4.1: k-Means with the standardized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e7be2-1756-4e44-9bcd-56a0fa7c78b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:12:39.212022Z",
     "start_time": "2024-12-21T17:12:39.032324Z"
    }
   },
   "outputs": [],
   "source": [
    "#we make a comparison between results with different k\n",
    "\n",
    "km_std_list = []\n",
    "silcoeff_std_list = []\n",
    "k_std_list = list(range(3, 11))\n",
    "\n",
    "# START THE FOR-CYCLE TO RUN THE k-MEANS AND MEASURING THE SILHOUETTE COEFFICIENT\n",
    "for i in range(len(k_std_list)):\n",
    "    print(f'****************** START k-MEANS WITH k={k_std_list[i]} ******************')\n",
    "    print('Computing...')\n",
    "    km_std_list.append(KMeans(n_clusters=k_std_list[i], random_state = 47 ))\n",
    "    km = km_std_list[i]\n",
    "    km.fit(Y_std)\n",
    "    silcoeff_std_list.append(silhouette_score(Y_std, labels = km.labels_))\n",
    "    print(f'****************** END k-MEANS WITH k={k_std_list[i]} ******************')\n",
    "    print('')\n",
    "\n",
    "# FIND THE BEST VALUE OF k AND THE BEST KMeans OBJECT\n",
    "i_best_std = np.argmax(silcoeff_std_list)\n",
    "k_std = k_std_list[i_best_std]\n",
    "km_std = km_std_list[i_best_std]\n",
    "\n",
    "# VISUALIZE THE RESULT\n",
    "print('')\n",
    "print('')\n",
    "print('****************** RESULTS OF THE SEARCH... ******************')\n",
    "print(f'BEST SILHOUETTE SCORE: {np.max(silcoeff_std_list)} --> k = {k_std}')\n",
    "print('**************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we plot the centroids into the 3D score graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%matplotlib widget\n",
    "#plt.ion()\n",
    "\n",
    "sg_std_k1 = plt.figure(figsize=(12, 12))\n",
    "ax_sg_std_k1 = sg_std_k1.add_subplot(111, projection='3d')\n",
    "ax_sg_std_k1.scatter(Y_std[:,0],Y_std[:,1],Y_std[:,2],  c=km_std.labels_, cmap='viridis', s=30, alpha=0.6)\n",
    "plt.title('Standardized dataset, KMeans - SCORE GRAPH')\n",
    "ax_sg_std_k1.set_xlabel(pc_std_names[0])\n",
    "ax_sg_std_k1.set_ylabel(pc_std_names[1])\n",
    "ax_sg_std_k1.set_zlabel(pc_std_names[2])\n",
    "ax_sg_std_k1.scatter(\n",
    "    km_std.cluster_centers_[:, 0], km_std.cluster_centers_[:, 1], km_std.cluster_centers_[:, 2],\n",
    "    c='red', marker='*', s=200, alpha=0.8,\n",
    "    label='Centroids'\n",
    ")\n",
    "plt.grid()\n",
    "for kk in range(k_std):\n",
    "    ax_sg_std_k1.text(km_std.cluster_centers_[kk, 0], km_std.cluster_centers_[kk, 1], km_std.cluster_centers_[kk, 2], f'clust.{kk + 1}')\n",
    "\n",
    "plt.show()\n",
    "#input()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "\n",
    "axs[0].scatter(Y_std[:, 1], Y_std[:, 2], c=km_std.labels_, cmap='viridis', s=20, alpha=0.65)\n",
    "axs[0].set_title('Standardized dataset - 2D SCORE GRAPH')\n",
    "axs[0].set_xlabel(pc_std_names[1])  # PC2\n",
    "axs[0].set_ylabel(pc_std_names[2])  # PC3\n",
    "axs[0].scatter(\n",
    "    km_std.cluster_centers_[:, 1], km_std.cluster_centers_[:, 2],\n",
    "    c='red', marker='*', s=200, alpha=0.8, label='Centroids'\n",
    ")\n",
    "\n",
    "for kk in range(k_std):\n",
    "    axs[0].text(\n",
    "        km_std.cluster_centers_[kk, 1], \n",
    "        km_std.cluster_centers_[kk, 2],\n",
    "        f'clust.{kk + 1}'\n",
    "    )\n",
    "\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "axs[1].scatter(Y_std[:, 0], Y_std[:, 2], c=km_std.labels_, cmap='viridis', s=20, alpha=0.65)\n",
    "axs[1].set_title('Standardized dataset - 2D SCORE GRAPH')\n",
    "axs[1].set_xlabel(pc_std_names[0])  # PC1\n",
    "axs[1].set_ylabel(pc_std_names[2])  # PC3\n",
    "axs[1].scatter(\n",
    "    km_std.cluster_centers_[:, 0], km_std.cluster_centers_[:, 2],\n",
    "    c='red', marker='*', s=200, alpha=0.8, label='Centroids'\n",
    ")\n",
    "\n",
    "for kk in range(k_std):\n",
    "    axs[1].text(\n",
    "        km_std.cluster_centers_[kk, 0], \n",
    "        km_std.cluster_centers_[kk, 2],\n",
    "        f'clust.{kk + 1}'\n",
    "    )\n",
    "\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "\n",
    "axs[2].scatter(Y_std[:, 0], Y_std[:, 1], c=km_std.labels_, cmap='viridis', s=20, alpha=0.65)\n",
    "axs[2].set_title('Standardized dataset - 2D SCORE GRAPH')\n",
    "axs[2].set_xlabel(pc_std_names[0])  # PC1\n",
    "axs[2].set_ylabel(pc_std_names[1])  # PC2\n",
    "axs[2].scatter(\n",
    "    km_std.cluster_centers_[:, 0], km_std.cluster_centers_[:, 1],\n",
    "    c='red', marker='*', s=200, alpha=0.8, label='Centroids'\n",
    ")\n",
    "\n",
    "for kk in range(k_std):\n",
    "    axs[2].text(\n",
    "        km_std.cluster_centers_[kk, 0], \n",
    "        km_std.cluster_centers_[kk, 1],\n",
    "        f'clust.{kk + 1}'\n",
    "    )\n",
    "\n",
    "axs[2].grid(True)\n",
    "axs[2].legend()\n",
    "\n",
    "# Aggiusta gli spazi tra i sottoplot per evitare sovrapposizioni di testi\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:07:17.229798Z",
     "start_time": "2024-12-21T18:07:15.392491Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_y_std = Y_std.max(axis=0)\n",
    "min_y_std = Y_std.min(axis=0)\n",
    "\n",
    "\n",
    "fig_centroids_std, ax_centroids_std = plt.subplots(1, 4, figsize=(14, 10))\n",
    "ax_centroids_std = ax_centroids_std.flatten()\n",
    "\n",
    "for ii in range(k_std):\n",
    "    ax_centroids_std[ii].bar(np.arange(km_std.cluster_centers_.shape[1]), max_y_std, color='blue', alpha=0.15)\n",
    "    ax_centroids_std[ii].bar(np.arange(km_std.cluster_centers_.shape[1]), min_y_std, color='blue', alpha=0.15)\n",
    "    ax_centroids_std[ii].bar(np.arange(km_std.cluster_centers_.shape[1]), km_std.cluster_centers_[ii, :])\n",
    "    ax_centroids_std[ii].set_xticks(ticks=np.arange(km_std.cluster_centers_.shape[1]))\n",
    "    ax_centroids_std[ii].set_xticklabels(labels= pc_std_names, rotation=45, ha='right')\n",
    "    ax_centroids_std[ii].grid(visible=True, which='both')\n",
    "    ax_centroids_std[ii].set_title(f'Standardized dataset - CENTROID {ii+1}')\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713d24a",
   "metadata": {},
   "source": [
    "#### **Centroids/Clusters** Interpretation:\n",
    " **Cluster 1 - Introvert/NotCultured/MildlySelfControlled:** Both Centroid and points visualization leeds to consider individual belonging to this cluster as **scarcely interested in Arts and Culture**, **Introvert** and mildly **Rational and Self-Controlled**  \n",
    "\n",
    " **Cluster 2 - Intr&Extr/Cultured/SelfContr:** Both Centroid and points visualization leeds to consider individual belonging to this cluster as **largely interested in Arts and Culture** and **Self-Controlled** \n",
    "\n",
    " **Cluster 3 - Cult&NotCu/Extrovert/Impressionable:** Both Centroid and points visualization leeds to consider individual belonging to this cluster as **largely Impressionable** and mildly **Extrovert** \n",
    "\n",
    " **Cluster 4 - NotCult/Extrovert/Rational:** Both Centroid and points visualization leeds to consider individual belonging to this cluster as **scarcely interested in Arts and Culture**, **Extrovert** and **Rational and Self-Controlled** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4da4b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_std_names = ['NotCultured/MildlySelfControlled/Introvert', 'Cultured/SelfContr/Intr&Extr', 'Cult&NotCu/Impressionable/Extrovert', 'NotCult/Rational/Extrovert']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **Section 4.2**: k-Means with the MinMax dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:26:43.362901Z",
     "start_time": "2024-12-21T17:26:43.190915Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we make a comparison between results with different k\n",
    "\n",
    "km_mm_list = []\n",
    "silcoeff_mm_list = []\n",
    "k_mm_list = list(range(3, 11))\n",
    "\n",
    "# START THE FOR-CYCLE TO RUN THE k-MEANS AND MEASURING THE SILHOUETTE COEFFICIENT\n",
    "for i in range(len(k_mm_list)):\n",
    "    print(f'****************** START k-MEANS WITH k={k_mm_list[i]} ******************')\n",
    "    print('Computing...')\n",
    "    km_mm_list.append(KMeans(n_clusters=k_mm_list[i], random_state = 47))\n",
    "    km = km_mm_list[i]\n",
    "    km.fit(Y_mm)\n",
    "    silcoeff_mm_list.append(silhouette_score(Y_mm, labels = km.labels_))\n",
    "    print(f'****************** END k-MEANS WITH k={k_mm_list[i]} ******************')\n",
    "    print('')\n",
    "\n",
    "# FIND THE BEST VALUE OF k AND THE BEST KMeans OBJECT\n",
    "i_best_mm = np.argmax(silcoeff_mm_list)\n",
    "k_mm = k_mm_list[i_best_std]\n",
    "km_mm = km_mm_list[i_best_std]\n",
    "\n",
    "# VISUALIZE THE RESULT\n",
    "print('')\n",
    "print('')\n",
    "print('****************** RESULTS OF THE SEARCH... ******************')\n",
    "print(f'BEST SILHOUETTE SCORE: {np.max(silcoeff_mm_list)} --> k = {k_mm}')\n",
    "print('**************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:28:42.436910Z",
     "start_time": "2024-12-21T17:28:42.149356Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#%matplotlib widget\n",
    "#plt.ion()\n",
    "\n",
    "sg_mm_k1 = plt.figure(figsize=(13, 13))\n",
    "ax_sg_mm_k1 = sg_mm_k1.add_subplot(111, projection='3d')\n",
    "ax_sg_mm_k1.scatter(Y_mm[:,0],Y_mm[:,1],Y_mm[:,2], c=km_mm.labels_, cmap='viridis', s=50, alpha=0.6)\n",
    "plt.title('MinMax dataset, KMeans - SCORE GRAPH')\n",
    "ax_sg_mm_k1.set_xlabel(pc_mm_names[0])\n",
    "ax_sg_mm_k1.set_ylabel(pc_mm_names[1])\n",
    "ax_sg_mm_k1.set_zlabel(pc_mm_names[2])\n",
    "ax_sg_mm_k1.scatter(\n",
    "    km_mm.cluster_centers_[:, 0], km_mm.cluster_centers_[:, 1], km_mm.cluster_centers_[:, 2],\n",
    "    c='red', marker='*', s=200, alpha=1,\n",
    "    label='Centroids'\n",
    ")\n",
    "\n",
    "plt.grid()\n",
    "for kk in range(k_std):\n",
    "    ax_sg_mm_k1.text(km_mm.cluster_centers_[kk, 0], km_mm.cluster_centers_[kk, 1], km_mm.cluster_centers_[kk, 2], f'clust.{kk + 1}')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#input()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "\n",
    "axs[0].scatter(Y_mm[:, 1], Y_mm[:, 2], c=km_mm.labels_, cmap='viridis', s=20, alpha=0.65)\n",
    "axs[0].set_title('Standardized dataset - 2D SCORE GRAPH')\n",
    "axs[0].set_xlabel(pc_mm_names[1])  # PC2\n",
    "axs[0].set_ylabel(pc_mm_names[2])  # PC3\n",
    "axs[0].scatter(\n",
    "    km_mm.cluster_centers_[:, 1], km_mm.cluster_centers_[:, 2],\n",
    "    c='red', marker='*', s=200, alpha=0.8, label='Centroids'\n",
    ")\n",
    "\n",
    "for kk in range(k_std):\n",
    "    axs[0].text(\n",
    "        km_mm.cluster_centers_[kk, 1], \n",
    "        km_mm.cluster_centers_[kk, 2],\n",
    "        f'clust.{kk + 1}'\n",
    "    )\n",
    "\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "axs[1].scatter(Y_mm[:, 0], Y_mm[:, 2], c=km_mm.labels_, cmap='viridis', s=20, alpha=0.65)\n",
    "axs[1].set_title('Standardized dataset - 2D SCORE GRAPH')\n",
    "axs[1].set_xlabel(pc_mm_names[0])  # PC1\n",
    "axs[1].set_ylabel(pc_mm_names[2])  # PC3\n",
    "axs[1].scatter(\n",
    "    km_mm.cluster_centers_[:, 0], km_mm.cluster_centers_[:, 2],\n",
    "    c='red', marker='*', s=200, alpha=0.8, label='Centroids'\n",
    ")\n",
    "\n",
    "for kk in range(k_std):\n",
    "    axs[1].text(\n",
    "        km_mm.cluster_centers_[kk, 0], \n",
    "        km_mm.cluster_centers_[kk, 2],\n",
    "        f'clust.{kk + 1}'\n",
    "    )\n",
    "\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "\n",
    "axs[2].scatter(Y_mm[:, 0], Y_mm[:, 1], c=km_mm.labels_, cmap='viridis', s=20, alpha=0.65)\n",
    "axs[2].set_title('Standardized dataset - 2D SCORE GRAPH')\n",
    "axs[2].set_xlabel(pc_mm_names[0])  # PC1\n",
    "axs[2].set_ylabel(pc_mm_names[1])  # PC2\n",
    "axs[2].scatter(\n",
    "    km_mm.cluster_centers_[:, 0], km_mm.cluster_centers_[:, 1],\n",
    "    c='red', marker='*', s=200, alpha=0.8, label='Centroids'\n",
    ")\n",
    "\n",
    "for kk in range(k_std):\n",
    "    axs[2].text(\n",
    "        km_mm.cluster_centers_[kk, 0], \n",
    "        km_mm.cluster_centers_[kk, 1],\n",
    "        f'clust.{kk + 1}'\n",
    "    )\n",
    "\n",
    "axs[2].grid(True)\n",
    "axs[2].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T18:10:36.784006Z",
     "start_time": "2024-12-21T18:10:36.328964Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_y_mm = Y_mm.max(axis=0)\n",
    "min_y_mm = Y_mm.min(axis=0)\n",
    "\n",
    "\n",
    "fig_centroids_mm, ax_centroids_mm = plt.subplots(1, 4, figsize=(14, 10))\n",
    "ax_centroids_mm = ax_centroids_mm.flatten()\n",
    "\n",
    "for ii in range(k_mm):\n",
    "    ax_centroids_mm[ii].bar(np.arange(km_mm.cluster_centers_.shape[1]), max_y_mm, color='blue', alpha=0.15)\n",
    "    ax_centroids_mm[ii].bar(np.arange(km_mm.cluster_centers_.shape[1]), min_y_mm, color='blue', alpha=0.15)\n",
    "    ax_centroids_mm[ii].bar(np.arange(km_mm.cluster_centers_.shape[1]), km_mm.cluster_centers_[ii, :])\n",
    "    ax_centroids_mm[ii].set_xticks(ticks=np.arange(km_mm.cluster_centers_.shape[1]))\n",
    "    ax_centroids_mm[ii].set_xticklabels(labels= pc_mm_names, rotation=45, ha = 'right')\n",
    "    ax_centroids_mm[ii].grid(visible=True, which='both')\n",
    "    ax_centroids_mm[ii].set_title(f'MinMax dataset - CENTROID {ii+1}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b839082f",
   "metadata": {},
   "source": [
    "#### **Centroids/Clusters** Interpretation:\n",
    " **Cluster 1 - NotCultured/MildlyRational/NotProactive:** Both Centroid and points visualization leeds to consider individual belonging to this cluster as **scarcely interested in Arts and Culture**, **not Proactive** and mildly **Rational and Self-Controlled**  \n",
    "\n",
    " **Cluster 2 - Cultured/MildlyRational/MildlyProactive:** Both Centroid and points visualization leeds to consider individual belonging to this cluster as **largely interested in Arts and Culture** and **Rational/Alternative** \n",
    "\n",
    " **Cluster 3 - MildlyNotCult/Impressionable/MildlyProactive:** Both Centroid and points visualization leeds to consider individual belonging to this cluster as **largely Impressionable** and mildly **Proactive** and interested to **Culture**\n",
    "\n",
    " **Cluster 4 - NotCult/Rational/Proactive:** Both Centroid and points visualization leeds to consider individual belonging to this cluster as **scarcely interested in Arts and Culture**, **Rational** and **Proactive** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bc3931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mm_names = ['NotCultured/MildlyRational/NotProactive', 'Cultured/MildlyRational/MildlyProactive', 'MildlyNotCult/Impressionable/MildlyProactive', 'NotCult/Rational/Proactive']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165c3e8-1dbb-46c0-b09e-d7704464c324",
   "metadata": {},
   "source": [
    "## Exercise 5. Cluster Evaluations\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), perform an **external evaluation** of the clustering obtained at exercise 4.1 with respect to one or more labels in the list _labels_. **Comment the results, comparing the evaluation with the interpretation you gave at exercise 4.3**. \n",
    "2. For each one of the two datasets (_std_ and _mm_), perform an **internal evaluation** of each cluster, with respect to the silohuette score. **Comment the results**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **Section 5.1: External evaluation of the Standardized dataset**\n",
    "\n",
    "#### As we saw in the 2D Score Plots, often one single lable is not explicative of the records distribution since the characteristics of an individual considered in the dataset are many and almost each one is significative. Fortunately also the Lables where assorted, so we thought about exploring the possibility of composing more complex and meaningfull lables just assembilng the simple ones.  We start with an analisys of the clustering using the simple lables, then we analize compare the clusterings obtained with a CompleX Lable that takes into account: \n",
    "#### - Age: The age interval has been discretized to Young (<= 19) and Older (>= 20). This choice aligned to the distribution of the labels (19 is the mean value) and the information considered in the analisys regarding the education.\n",
    "#### - Education: The label has been partitioned in two groups, one with people that attended university and an other with people that didn't.\n",
    "#### - Gender.\n",
    "#### - Other Simple lables have been added to the Complex label like 'Onli Child' or 'Town' but despite adding more complexity did not really add much meaning.\n",
    "---\n",
    "#### **Adjusted Rand Index:** The Adjusted Rand Index (ARI) is a statistical measure used to evaluate the similarity between two data clusterings, correcting for chance agreement. It quantifies the alignment of clustering results by comparing the number of pairwise agreements and disagreements. ARI ranges from -1 (no similarity) to 1 (perfect agreement), where 0 indicates random labeling. We used it to have also a quantitative measure of the clusters quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef57ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_filt\n",
    "variables_by_type[\"Demographics\"]\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    homogeneity_score,\n",
    "    completeness_score,\n",
    "    v_measure_score\n",
    ")\n",
    "\n",
    "\"\"\"     resp_filt[\"Age\"].loc[~mask]\n",
    "    (km_mm.labels_)[~mask] \"\"\"\n",
    "\n",
    "\n",
    "for labeling in variables_by_type[\"Demographics\"]:\n",
    "\n",
    "    mask = resp_filt[labeling].isna()\n",
    "\n",
    "    ari = adjusted_rand_score(resp_filt[labeling].loc[~mask], (km_std.labels_)[~mask])\n",
    "\n",
    "    # 2. Homogeneity Score\n",
    "    homogeneity = homogeneity_score(resp_filt[labeling].loc[~mask], (km_std.labels_)[~mask])\n",
    "\n",
    "    # 3. Completeness Score\n",
    "    completeness = completeness_score(resp_filt[labeling].loc[~mask], (km_std.labels_)[~mask])\n",
    "\n",
    "    # 4. V-Measure\n",
    "    v_measure = v_measure_score(resp_filt[labeling].loc[~mask], (km_std.labels_)[~mask])\n",
    "\n",
    "    # Print the external evaluation metrics\n",
    "\n",
    "    print(labeling)\n",
    "    print(\"External Evaluation Metrics:\")\n",
    "    print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
    "    print(f\"Homogeneity Score:  {homogeneity:.3f}\")\n",
    "    print(f\"Completeness Score: {completeness:.3f}\")\n",
    "    print(f\"V-Measure:          {v_measure:.3f}\")\n",
    "    print(f\"-----------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babe2f4-f800-4f51-9c97-6059d4ca12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for labeling in variables_by_type[\"Demographics\"]:\n",
    "\n",
    "    mask = resp_filt[labeling].isna()\n",
    "\n",
    "    ari = adjusted_rand_score(resp_filt[labeling].loc[~mask], (km_mm.labels_)[~mask])\n",
    "\n",
    "    # 2. Homogeneity Score\n",
    "    homogeneity = homogeneity_score(resp_filt[labeling].loc[~mask], (km_mm.labels_)[~mask])\n",
    "\n",
    "    # 3. Completeness Score\n",
    "    completeness = completeness_score(resp_filt[labeling].loc[~mask], (km_mm.labels_)[~mask])\n",
    "\n",
    "    # 4. V-Measure\n",
    "    v_measure = v_measure_score(resp_filt[labeling].loc[~mask], (km_mm.labels_)[~mask])\n",
    "\n",
    "    # Print the external evaluation metrics\n",
    "\n",
    "    print(labeling)\n",
    "    print(\"External Evaluation Metrics:\")\n",
    "    print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
    "    print(f\"Homogeneity Score:  {homogeneity:.3f}\")\n",
    "    print(f\"Completeness Score: {completeness:.3f}\")\n",
    "    print(f\"V-Measure:          {v_measure:.3f}\")\n",
    "    print(f\"-----------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49206b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Functions to create new labels\n",
    "#  \n",
    "def attended_university(edu_str):\n",
    "    \"\"\"\n",
    "    Returns 'University' if the education string corresponds to\n",
    "    any post-secondary or university-level degree,\n",
    "    otherwise 'NoUniversity'.\n",
    "    \"\"\"\n",
    "    if pd.isna(edu_str):\n",
    "        return 'UnknownUniv'\n",
    "    \n",
    "    uni_levels = {\n",
    "        'college/bachelor degree',\n",
    "        'masters degree',\n",
    "        'doctorate degree'\n",
    "        # aggiungi/rimuovi altri gradi di istruzione come necessario\n",
    "    }\n",
    "    return 'University' if edu_str in uni_levels else 'NoUniversity'\n",
    "\n",
    "def only_child(only_str):\n",
    "    \"\"\"\n",
    "    Returns 'Only' if the guy is only child.\n",
    "    \"\"\"\n",
    "    if only_str == 'yes':\n",
    "        return 'OnlyC'\n",
    "    \n",
    "    if only_str == 'no':\n",
    "        return 'NotOnlyC'\n",
    "    else:\n",
    "        return 'UnkC'\n",
    "    \n",
    "                   \n",
    "    return 'University' if edu_str in uni_levels else 'NoUniversity'\n",
    "\n",
    "def age_group(age):\n",
    "    \"\"\"\n",
    "    Groups 'Age' into categories.\n",
    "    Customize the boundaries or labels as needed.\n",
    "    \"\"\"\n",
    "    if pd.isna(age):\n",
    "        return 'UnknownAge'\n",
    "    if age <= 19:\n",
    "        return 'Young'\n",
    "    elif 20 <= age:\n",
    "        return 'Older'\n",
    "    else:\n",
    "        return 'UnknownAge'  # nel caso servisse una categoria ulteriore\n",
    "\n",
    "def create_complex_label_with_gender(row):\n",
    "    \"\"\"\n",
    "    Creates a label: \"<AgeGroup>-<Gender>-<UnivAttended>\"\n",
    "    Example: \"Young-male-NoUniversity\"\n",
    "    \"\"\"\n",
    "    if pd.isna(row['Gender']):\n",
    "        gender_label = 'UnknownGender'\n",
    "    else:\n",
    "        gender_label = row['Gender']  # 'male', 'female', ecc.\n",
    "    #return f\"{row['AgeGroup']}-{gender_label}-{row['UnivAttended']}-{row['OnlyCh']}\"\n",
    "    return f\"{row['AgeGroup']}-{gender_label}-{row['UnivAttended']}\"\n",
    "\n",
    "def create_complex_label_no_gender(row):\n",
    "    \"\"\"\n",
    "    Creates a label: \"<AgeGroup>-<UnivAttended>\"\n",
    "    Example: \"Young-NoUniversity\"\n",
    "    \"\"\"\n",
    "    #return f\"{row['AgeGroup']}-{row['UnivAttended']}-{row['OnlyCh']}\"\n",
    "    return f\"{row['AgeGroup']}-{row['UnivAttended']}\"\n",
    "\n",
    "\n",
    "# To plot the label distribution by cluster\n",
    "\n",
    "def plot_label_distribution_per_cluster(df, cluster_labels, label_col_name, cluster_names):\n",
    "    temp_df = pd.DataFrame({\n",
    "        'cluster': cluster_labels,\n",
    "        'label': df[label_col_name]\n",
    "    })\n",
    "\n",
    "    group_data = (\n",
    "        temp_df\n",
    "        .groupby(['cluster', 'label'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    unique_clusters = group_data['cluster'].unique()\n",
    "    num_clusters = len(unique_clusters)\n",
    "\n",
    "    # Create subplots for all clusters\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_clusters, figsize=(7 * num_clusters, 5), sharey=True)\n",
    "    if num_clusters == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable if there's only one subplot\n",
    "\n",
    "    for idx, c in enumerate(unique_clusters):\n",
    "        cluster_subset = group_data[group_data['cluster'] == c]\n",
    "\n",
    "        ax = axes[idx]\n",
    "        ax.bar(cluster_subset['label'].astype(str), cluster_subset['count'])\n",
    "        ax.set_title(cluster_names[c])\n",
    "        ax.set_xlabel(label_col_name)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Creating the complex labels and adding them to main DataFrame\n",
    "\n",
    "resp_filt['UnivAttended'] = resp_filt['Education'].apply(attended_university)\n",
    "resp_filt['AgeGroup'] = resp_filt['Age'].apply(age_group)\n",
    "resp_filt['ComplexLabel_WithGender'] = resp_filt.apply(create_complex_label_with_gender, axis=1)\n",
    "resp_filt['ComplexLabel_NoGender'] = resp_filt.apply(create_complex_label_no_gender, axis=1)\n",
    "\n",
    "#print(\"\\nFinal DataFrame:\\n\")\n",
    "#print(resp_filt.head())\n",
    "\n",
    "\n",
    "for [cluster_labels, cluster_names] in [[km_std.labels_, cluster_std_names], [km_mm.labels_, cluster_mm_names]]:\n",
    "\n",
    "    #Computing the Adjusted Rand Index\n",
    "\n",
    "\n",
    "    mask_dg = resp_filt['Gender'].str.contains('Unknown', na=True)\n",
    "    ari_dg = adjusted_rand_score(resp_filt['Gender'][~mask_dg],\n",
    "                                 cluster_labels[~mask_dg])\n",
    "    print(\"\\n--- External Evaluation with Gender ---\")\n",
    "    print(f\"Adjusted Rand Index (ARI): {ari_dg:.3f}\")\n",
    "\n",
    "    mask_un = resp_filt['UnivAttended'].str.contains('Unknown', na=True)\n",
    "    ari_un = adjusted_rand_score(resp_filt['UnivAttended'][~mask_un],\n",
    "                                 cluster_labels[~mask_un])\n",
    "    print(\"\\n--- External Evaluation with UnivAttended ---\")\n",
    "    print(f\"Adjusted Rand Index (ARI): {ari_un:.3f}\")\n",
    "    \n",
    "    \n",
    "    # Handling the Missing values in the labels by discarding the record\n",
    "    mask_ng = resp_filt['ComplexLabel_NoGender'].str.contains('Unknown', na=True)\n",
    "    ari_ng = adjusted_rand_score(resp_filt['ComplexLabel_NoGender'][~mask_ng],\n",
    "                                 cluster_labels[~mask_ng])\n",
    "    print(\"\\n--- External Evaluation with ComplexLabel_NoGender ---\")\n",
    "    print(f\"Adjusted Rand Index (ARI): {ari_ng:.3f}\")\n",
    "\n",
    "    mask_wg = resp_filt['ComplexLabel_WithGender'].str.contains('Unknown', na=True)\n",
    "    ari_wg = adjusted_rand_score(resp_filt['ComplexLabel_WithGender'][~mask_wg],\n",
    "                                 cluster_labels[~mask_wg])\n",
    "    print(\"\\n--- External Evaluation with ComplexLabel_WithGender ---\")\n",
    "    print(f\"Adjusted Rand Index (ARI): {ari_wg:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting the Histogram of lables distribution in clusters\n",
    "\n",
    "    plot_label_distribution_per_cluster(\n",
    "        df=resp_filt,\n",
    "        cluster_labels=cluster_labels,\n",
    "        label_col_name='Age',\n",
    "        cluster_names = cluster_names\n",
    "    )\n",
    "\n",
    "    plot_label_distribution_per_cluster(\n",
    "        df=resp_filt,\n",
    "        cluster_labels=cluster_labels,\n",
    "        label_col_name='Gender',\n",
    "        cluster_names = cluster_names\n",
    "    )\n",
    "\n",
    "    plot_label_distribution_per_cluster(\n",
    "        df=resp_filt,\n",
    "        cluster_labels=cluster_labels,\n",
    "        label_col_name='UnivAttended',\n",
    "        cluster_names = cluster_names\n",
    "    )\n",
    "\n",
    "\n",
    "    plot_label_distribution_per_cluster(\n",
    "        df=resp_filt,\n",
    "        cluster_labels=cluster_labels,\n",
    "        label_col_name='ComplexLabel_NoGender',\n",
    "        cluster_names = cluster_names\n",
    "    )\n",
    "\n",
    "    plot_label_distribution_per_cluster(\n",
    "        df=resp_filt,\n",
    "        cluster_labels=cluster_labels,\n",
    "        label_col_name='ComplexLabel_WithGender',\n",
    "        cluster_names = cluster_names\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d6e313",
   "metadata": {},
   "source": [
    "#### **Interpretation** ####\n",
    "\n",
    "The situation delined is quite interesting. Assuming that the Clusters meaning is similar (as it is) for the MinMax and Std dataframes, we can see that the lables are distributer almost equally among the cluster in the two situations; the ARI insignificantly higher for the MinMax df, but can be considered equal.\n",
    "\n",
    "---\n",
    "\n",
    "- The **Age** does not seems to do be a key indicator expressed from the clusters, while the **Geneder** highlights an high concentrations of females in the *Cult&NotCult/Impressionable/Extrover* and of males in the *NotCult/Rational/Extrovert*. Regarding the first cluster we can say that is almos equally populated by males and females, while the second still presents a strong polarization respect to the females.\n",
    "---\n",
    "- If now we look at the **Complex Label Age-University** we can see that the last two clusters, the *Cult&NotCult/Impressionable/Extrover* and *NotCultured/Rational/Extrover* are populated mainly by individuals that did not attend university either because too young (<=19) or because not interested (>19), the first two clusters instead presents a more mixed population of people that did and didn't. \n",
    " **University attendance** seems a discriminant factor but probably, as we can see from the Age's distribution, the majority of the sampled population were to young to attended it, so the lable is strongly unbalanced in the dataset and the class of people who did attend is Underepresented.\n",
    "---\n",
    "- An ulterior degree of insight can be gained if we look a third **Complex Label**, that mixes **Gender-Age-University**: \n",
    "- The *NotCultured/Rational/Extrover* cluster is mainly populed of males who didn't attend University either because they are too young or for other reasons. \n",
    "- The *Cult&NotCult/Impressionable/Extrovert* Cluster is mainly populated from females that did not attend university, both young and old. There is also a significant third population of females that did attend University. \n",
    "- The *Cultured/SelfControl/Intro&Extrovert* Cluster is composed from Older people that both attended and did not attend university and that from both genders in the same measure. Among the young ones, the only group that have a strong presence in the cluster is the Young females that did not attend University. \n",
    "- Regarding the first cluster *NotCultured/MildlySelfcontrolled/Introvert* we can see that presents a distribution similar to the second one, but is less populated. The categories that are more expressed in the cluster are Older Males that didn't attend University and Young females that didn't attend as well.\n",
    "---\n",
    "#### **Conclusions** #### \n",
    "\n",
    "The results for the bot dataset are identical, except that in Min-Max situation, the distributions seem to tend more towards specific classes, allowing for a clearer distinction. In contrast, in Standardized case, the classes in the dataset appear more balanced and mixed. Although it is not possible to perfectly assign specific labels, whether simple or complex, to the clusters, it was still possible, using the complex labels, to interpret the results to some extent. This allowed for a meaningful correlation between the complex significance of the labels and the clusters identified by PCA in the data.The dimensionality of the feature space, combined with the relatively uniform significance of each feature, plays a crucial role in the challenges associated with analyzing this dataset. (VEDERE SE DIRE DI PIU ES MASCHI/CULTURA DONNE IMPRESSIONABILITA ETC.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0e083",
   "metadata": {},
   "source": [
    "### Section 5.2 **Internal Evaluation** ###\n",
    "- First we define a function to perform Silhouette analisys computing both (avg) global and cluster's Silhouette, the we apply it to the two clusterings and comment the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f6713a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "def silhouette_analysis(X, labels):\n",
    "\n",
    "    # Computes the Global Silhouette\n",
    "    silhouette_avg = silhouette_score(X, labels)\n",
    "    print(\"Mean Silhouette (Global):\", silhouette_avg)\n",
    "    \n",
    "    # Computes the silhouett sample by sample\n",
    "    sample_silhouette_values = silhouette_samples(X, labels)\n",
    "    \n",
    "    # \n",
    "    cluster_ids = np.unique(labels)\n",
    "    n_clusters = len(cluster_ids)\n",
    "    \n",
    "    # Prepariamo la figura con due sottoplot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    y_lower = 10\n",
    "    cmap = cm.nipy_spectral\n",
    "\n",
    "    for i, cluster_id in enumerate(cluster_ids):\n",
    "        # Estrae i valori di silhouette di tutti i campioni del cluster i\n",
    "        ith_cluster_sil_values = sample_silhouette_values[labels == cluster_id]\n",
    "        \n",
    "        # Li ordina in modo crescente \n",
    "        ith_cluster_sil_values.sort()\n",
    "        \n",
    "        # Numero di campioni nel cluster i\n",
    "        size_cluster_i = ith_cluster_sil_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        # Sceglie un colore per il cluster i\n",
    "        color = cmap(float(i) / n_clusters)\n",
    "        \n",
    "        # Disegna l'area corrispondente ai campioni del cluster i\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_sil_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        \n",
    "        # Scrive il nome (o id) del cluster sul grafico\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(cluster_id))\n",
    "        \n",
    "        # Aggiusta il limite inferiore per il cluster successivo\n",
    "        y_lower = y_upper + 10  \n",
    "    \n",
    "    # Disegna la linea verticale con la silhouette media globale\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", label=f\"Silhouette Avg = {silhouette_avg:.2f}\")\n",
    "\n",
    "    # Imposta etichette e titolo del grafico\n",
    "    ax1.set_title(\"Silhouette Analisys\")\n",
    "    ax1.set_xlabel(\"Silhouette Value\")\n",
    "    ax1.set_ylabel(\"Cluster\")\n",
    "    ax1.legend()\n",
    "\n",
    "\n",
    "    # Barplot\n",
    "\n",
    "    cluster_silhouette_means = []\n",
    "    for cluster_id in cluster_ids:\n",
    "        ith_cluster_sil_values = sample_silhouette_values[labels == cluster_id]\n",
    "        cluster_silhouette_means.append(np.mean(ith_cluster_sil_values))\n",
    "    \n",
    "    # \n",
    "    colors = [cmap(float(i) / n_clusters) for i in range(n_clusters)]\n",
    "    \n",
    "    ax2.bar(range(n_clusters),\n",
    "            cluster_silhouette_means,\n",
    "            color=colors,\n",
    "            edgecolor='black')\n",
    "    \n",
    "    ax2.set_title(\"Silhouette media per ciascun cluster\")\n",
    "    ax2.set_xlabel(\"Cluster\")\n",
    "    ax2.set_ylabel(\"Valore medio di silhouette\")\n",
    "    ax2.set_xticks(range(n_clusters))\n",
    "    ax2.set_xticklabels(cluster_ids) \n",
    "    \n",
    "    # Mostra la figura\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_analysis(Y_std, km_std.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_analysis(Y_mm, km_mm.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff02c6",
   "metadata": {},
   "source": [
    "\n",
    " The **internal evaluation** reports low values of the silhouette score in all the clusters. This means that the\n",
    "population inside the cluster have certain degree of difference. The low global silhouette value means that the clusters are not well separated, as we already noticed in section 3.4, where we could not distinguish the presence of clusters from simply plotting the points in the PCs space.\n",
    "These scores are also coherent with the score graph represented in exercise 4.2, since the four clusters represented from the 4 centroids were not so easily discernible by eye, without the use of colors and 2D scoreplots. The Global Silhouette is insignificantly higher for the Standardized df.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HW2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
