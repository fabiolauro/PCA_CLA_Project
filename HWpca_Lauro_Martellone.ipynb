{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6347f1d2-5cb0-4032-b637-4730e9b9383a",
   "metadata": {},
   "source": [
    "# Computational Linear Algebra: PCA Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905c56f-029b-4d04-9e68-739362350bd8",
   "metadata": {},
   "source": [
    "## Initialization:\n",
    "Fill the missing values in this text box and in the following code-cell.\n",
    "\n",
    "**Academic Year:** 2024/2025\n",
    "\n",
    "### Team Members (Alphabetical Order):\n",
    "1. Lauro, Fabio (s318121);\n",
    "2. Surname2, Name2 (StudentID2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c068e99-0c8c-433f-be0b-f41534dc8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "StudentID1 = 318121\n",
    "StudentID2 = 317331"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24f22b-a92f-4898-b26d-e4ab6d49c3d9",
   "metadata": {},
   "source": [
    "## Starting Code-Cell \n",
    "### Attention: DO NOT CHANGE THE CODE INSIDE THE FOLLOWING CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84734de0-62ad-4c39-b4b4-886696d3a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############## DO NOT CHANGE THE CODE IN THIS CELL #################\n",
    "####################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "var_entertainment_feat_types = ['Interests', 'Movies', 'Music']\n",
    "var_personal_feat_types = ['Finance', 'Phobias']\n",
    "fixed_feat_types = ['Personality', 'Health']\n",
    "\n",
    "label_types = ['Demographic']\n",
    "\n",
    "variables_by_type = {\n",
    "    'Demographics': ['Age', 'Height', 'Weight', 'Number of siblings', \n",
    "                     'Gender', 'Hand', 'Education', 'Only child', 'Home Town Type',\n",
    "                     'Home Type'],\n",
    "    'Finance': ['Finances', 'Shopping centres', 'Branded clothing', \n",
    "                'Entertainment spending', 'Spending on looks', \n",
    "                'Spending on gadgets', 'Spending on healthy eating'],\n",
    "    'Health': ['Smoking', 'Alcohol', 'Healthy eating'],\n",
    "    'Interests': ['History', 'Psychology', 'Politics', 'Mathematics', \n",
    "                  'Physics', 'Internet', 'PC', 'Economy Management', \n",
    "                  'Biology', 'Chemistry', 'Reading', 'Geography', \n",
    "                  'Foreign languages', 'Medicine', 'Law', 'Cars', \n",
    "                  'Art exhibitions', 'Religion', 'Countryside, outdoors', \n",
    "                  'Dancing', 'Musical instruments', 'Writing', 'Passive sport', \n",
    "                  'Active sport', 'Gardening', 'Celebrities', 'Shopping', \n",
    "                  'Science and technology', 'Theatre', 'Fun with friends', \n",
    "                  'Adrenaline sports', 'Pets'],\n",
    "    'Movies': ['Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', \n",
    "               'Sci-fi', 'War', 'Fantasy/Fairy tales', 'Animated', \n",
    "               'Documentary', 'Western', 'Action'],\n",
    "    'Music': ['Music', 'Slow songs or fast songs', 'Dance', 'Folk', \n",
    "              'Country', 'Classical music', 'Musical', 'Pop', 'Rock', \n",
    "              'Metal or Hardrock', 'Punk', 'Hiphop, Rap', 'Reggae, Ska', \n",
    "              'Swing, Jazz', 'Rock n roll', 'Alternative', 'Latino', \n",
    "              'Techno, Trance', 'Opera'],\n",
    "    'Personality': ['Daily events', 'Prioritising workload', \n",
    "                    'Writing notes', 'Workaholism', 'Thinking ahead', \n",
    "                    'Final judgement', 'Reliability', 'Keeping promises', \n",
    "                    'Loss of interest', 'Friends versus money', 'Funniness', \n",
    "                    'Fake', 'Criminal damage', 'Decision making', 'Elections', \n",
    "                    'Self-criticism', 'Judgment calls', 'Hypochondria', \n",
    "                    'Empathy', 'Eating to survive', 'Giving', \n",
    "                    'Compassion to animals', 'Borrowed stuff', \n",
    "                    'Loneliness', 'Cheating in school', 'Health', \n",
    "                    'Changing the past', 'God', 'Dreams', 'Charity', \n",
    "                    'Number of friends', 'Punctuality', 'Lying', 'Waiting', \n",
    "                    'New environment', 'Mood swings', 'Appearence and gestures', \n",
    "                    'Socializing', 'Achievements', 'Responding to a serious letter', \n",
    "                    'Children', 'Assertiveness', 'Getting angry', \n",
    "                    'Knowing the right people', 'Public speaking', \n",
    "                    'Unpopularity', 'Life struggles', 'Happiness in life', \n",
    "                    'Energy levels', 'Small - big dogs', 'Personality', \n",
    "                    'Finding lost valuables', 'Getting up', 'Interests or hobbies', \n",
    "                    \"Parents' advice\", 'Questionnaires or polls', 'Internet usage'],\n",
    "    'Phobias': ['Flying', 'Storm', 'Darkness', 'Heights', 'Spiders', 'Snakes', \n",
    "                'Rats', 'Ageing', 'Dangerous dogs', 'Fear of public speaking']\n",
    "}\n",
    "\n",
    "labels = variables_by_type['Demographics']\n",
    "\n",
    "try:\n",
    "    random_seed = min([StudentID1, StudentID2])\n",
    "except NameError:\n",
    "    random_seed = StudentID1\n",
    "\n",
    "def which_featgroups():\n",
    "    np.random.seed(random_seed)\n",
    "    these_entertainments = np.random.choice(var_entertainment_feat_types, 2, replace=False).tolist()\n",
    "    these_personal = np.random.choice(var_personal_feat_types, 1, replace=False).tolist()\n",
    "    these_types = fixed_feat_types + these_personal + these_entertainments\n",
    "    print('*** THESE ARE THE SELECTED TYPE OF VARIABLES:')\n",
    "    for k in these_types:\n",
    "        print(f'{k}')\n",
    "    print('*************************************')\n",
    "    return these_types\n",
    "\n",
    "def which_features(these_types):\n",
    "    np.random.seed(random_seed)\n",
    "    these_features = []\n",
    "    for type in these_types:\n",
    "        if type != 'Personality':\n",
    "            these_features += variables_by_type[type]\n",
    "        else:\n",
    "            these_features += np.random.choice(variables_by_type[type], \n",
    "                                               int(2 * (len(variables_by_type[type]) / 3)), \n",
    "                                               replace=False).tolist()\n",
    "    print('*** THESE ARE THE SELECTED FEATURES:')\n",
    "    for ft in these_features:\n",
    "        print(f'{ft}')\n",
    "    print('*************************************')\n",
    "    return these_features\n",
    "\n",
    "these_types = which_featgroups()\n",
    "these_features = which_features(these_types)\n",
    "\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c9090-6065-4f25-bdc3-1b3cdad6c083",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "In the following cell, import all the modules you think are necessary for doing the homework, **among the ones listed and used during the laboratories of the course**.\n",
    "No extra modules are allowed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad703d-e6d8-4239-8222-7aa91fbda3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT IMPORT NUMPY\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "import yaml\n",
    "from IPython.display import display  # to display variables in a \"nice\" way\n",
    "\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1143d0-e6a8-43dd-8bf8-0363842bac60",
   "metadata": {},
   "source": [
    "## Exercise 1. Preparing the Dataset\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. load the dataset \"_responses_hw.csv_\";\n",
    "2. create a working dataframe extracting from _responses_hw.csv_ the columns corresponding to the variables in _these_features_, and randomly selecting 2/3 of the rows. Let us call this dataframe _X_df_;\n",
    "3. analyze the obtained dataframe and performing cleansing/encoding operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2d477-033c-40fd-b31e-f7c03ddb2954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4058cb9-19fc-406c-8380-7e8053b43649",
   "metadata": {},
   "source": [
    "## Exercise 2. Analyzing the Variance and the PCs\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. create two new dataframes from _X_df_ applying a StandardScaler and a MinMaxscaler. Call these new dataframes as _Xstd_df_ and _Xmm_df_, respectively;\n",
    "2. compute the variance of all the features in _X_df_, _Xstd_df_, and _Xmm_df_ and **comment the results**;\n",
    "3. compute all the $n$ Principal Components (PCs) for each dataset _X_df_, _Xstd_df_, and _Xmm_df_. Then, visualize the curves of the cumulative explained variances and **comment the results**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181358a6-6bb0-4f22-b395-34e3757fa16e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14c1771a-09b2-41bc-bc4c-7e3f6f530021",
   "metadata": {},
   "source": [
    "## Exercise 3. Dimensionality Reduction and PC Interpretation\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two dataframes _Xstd_df_, and _Xmm_df_, compute a new PCA for performing a dimensionality reduction with respect to $m$ dimensions. The value of $m$ must be $$m = \\min\\{m', 5\\}\\,,$$ where $m'$ is the value required for obtaining $33\\%$ of the total variance.\n",
    "2. For both the cases, visualize all the PCs and give a name/interpretation to them. **Comment and motivate your interpretations**. If possible, **compare the differences among the results obtained** for _Xstd_df_ and _Xmm_df_.\n",
    "3. Perform the score graph for both the cases (_std_ and _mm_). If $m>3$, plot the score graph with respect to the first 3 PCs. All the **plots must show the names of the PCs on the axes** for better understanding the results.\n",
    "4. **Optional:** plot more score graphs, coloring the dots with respect to any label in the list _labels_ that you believe can be interesting. **Comment and analyze this optional plots**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c6d02-e58c-4db4-8872-d45c6eb9e4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94c8f0b8-37c5-403a-93ad-8abefd96e3b2",
   "metadata": {},
   "source": [
    "## Exercise 4. $k$-Means\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), run the $k$-Means for clustering the data. In particular, **use the silohuette score for identify the best value for $k\\in\\{3, \\ldots, 10\\}$**.\n",
    "2. Plot the score graphs of exercise 3.3, adding the centroids of the cluster.\n",
    "3. Observing the centroids coordinates in the PC space, **give a name/interpretation to them**, exploiting the names you assigned to the PCs. **Comment and motivate your interpretations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e7be2-1756-4e44-9bcd-56a0fa7c78b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c165c3e8-1dbb-46c0-b09e-d7704464c324",
   "metadata": {},
   "source": [
    "## Exercise 5. Cluster Evaluations\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), perform an **external evaluation** of the clustering obtained at exercise 4.1 with respect to one or more labels in the list _labels_. **Comment the results, comparing the evaluation with the interpretation you gave at exercise 4.3**. \n",
    "2. For each one of the two datasets (_std_ and _mm_), perform an **internal evaluation** of each cluster, with respect to the silohuette score. **Comment the results**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babe2f4-f800-4f51-9c97-6059d4ca12cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
